Got it âœ… Iâ€™ll create a **complete README.md (Markdown) for your GitHub repo** based on the project files you uploaded. This will look professional and explain the purpose, setup, and usage clearly.

Hereâ€™s the draft:

---

# ğŸ“š MCP Research Assistant

An AI-powered **research assistant** built on the **Model Context Protocol (MCP)**.
It can:

* ğŸ” Search academic papers from **arXiv**
* ğŸŒ Perform web searches (Tavily / DuckDuckGo fallback)
* ğŸ“„ Fetch & parse PDFs
* ğŸ“¥ Ingest documents into a **ChromaDB vector store**
* â“ Answer queries with semantic search over ingested papers
* ğŸ“‘ Generate citations (BibTeX, JSON)

This project integrates **OpenAI embeddings**, **ChromaDB**, and **MCP tooling** to create a research pipeline.

---

## âš¡ Features

* **arXiv Search** â†’ Find papers by query
* **Web Search** â†’ Uses Tavily API if available, otherwise DuckDuckGo
* **PDF Ingestion** â†’ Chunk & embed PDFs for semantic search
* **Vector Store** â†’ Store and query embeddings via ChromaDB
* **Citation Builder** â†’ Generate BibTeX citations from DOIs/URLs
* **MCP Server** â†’ Tools exposed for MCP-compatible clients (e.g., Claude Desktop)

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ server.py          # Main MCP server (FastMCP)
â”œâ”€â”€ embeddings.py      # Embeddings wrapper (OpenAI API)
â”œâ”€â”€ vectorstore.py     # ChromaDB vector store wrapper
â”œâ”€â”€ pdf_ingest.py      # PDF fetching, text extraction, chunking & ingestion
â”œâ”€â”€ arxiv_conn.py      # arXiv API connector
â”œâ”€â”€ websearch.py       # Tavily / DuckDuckGo web search connector
â”œâ”€â”€ citations.py       # Crossref citation builder
â””â”€â”€ requirements.txt   # Dependencies
```

---

## ğŸ”§ Installation

### 1. Clone the repo

```bash
git clone https://github.com/your-username/mcp-research-assistant.git
cd mcp-research-assistant
```

### 2. Create a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate   # On Linux/Mac
.venv\Scripts\activate      # On Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set environment variables

```bash
# Required
export OPENAI_API_KEY="your_openai_api_key"

# Optional (if you want Tavily web search)
export TAVILY_API_KEY="your_tavily_api_key"
```

---

## â–¶ï¸ Usage

### Run MCP server

```bash
python server.py stdio
```

This runs in **stdio mode**, ready for MCP-compatible clients like **Claude Desktop**.

---

## ğŸ› ï¸ Tools Exposed

| Tool                | Description                                 |
| ------------------- | ------------------------------------------- |
| `search_arxiv`      | Search arXiv for research papers            |
| `search_web`        | Search the web (Tavily/DuckDuckGo)          |
| `fetch_pdf`         | Download and parse a PDF                    |
| `ingest_folder`     | Ingest all PDFs from a folder               |
| `query_memory`      | Query stored documents with semantic search |
| `make_citations`    | Generate citations in BibTeX/JSON           |
| `reset_chroma_tool` | Reset the ChromaDB database                 |
| `ping`              | Test server response                        |

---

## ğŸ“‘ Example Workflows

### Search arXiv

```json
{
  "tool": "search_arxiv",
  "input": { "query": "machine learning", "max_results": 3 }
}
```

### Ingest PDFs

```json
{
  "tool": "ingest_folder",
  "input": { "path": "data/papers" }
}
```

### Query Knowledge Base

```json
{
  "tool": "query_memory",
  "input": { "question": "What is attention mechanism?", "top_k": 5 }
}
```

### Generate Citations

```json
{
  "tool": "make_citations",
  "input": { "doi_or_url_list": ["https://doi.org/10.48550/arXiv.1706.03762"], "style": "bibtex" }
}
```

---

## ğŸ“¦ Dependencies

* [OpenAI](https://pypi.org/project/openai/) â€“ embeddings
* [ChromaDB](https://www.trychroma.com/) â€“ vector database
* [PyMuPDF](https://pymupdf.readthedocs.io/en/latest/) â€“ PDF parsing
* [arxiv](https://pypi.org/project/arxiv/) â€“ arXiv API client
* [duckduckgo-search](https://pypi.org/project/duckduckgo-search/) â€“ fallback web search
* [requests](https://docs.python-requests.org/) â€“ HTTP requests
* [pydantic](https://docs.pydantic.dev/) â€“ schema validation
* [fastmcp](https://github.com/modelcontextprotocol/fastmcp) â€“ MCP server

---

## ğŸ“Œ Roadmap

* [ ] Add support for more citation styles
* [ ] Improve query relevance with RAG pipeline
* [ ] Support embeddings backends beyond OpenAI

---

## ğŸ¤ Contributing

Pull requests are welcome!
For major changes, please open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

---

Would you like me to also **generate a `requirements.txt`** for this project so your repo is fully ready to run?

